{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#크롤러 모델\n"
      ],
      "metadata": {
        "id": "_RB2IcvEl8dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#크롤링 모델 > 아이템 테이블 + 리뷰 테이블 통합 버전\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 데려올 카테고리 200000313 / 헤어케어 카테고리\n",
        "page = [1] # page를 더 추가가 가능함.\n",
        "p_id = 0 # product_id를 넣을 변수\n",
        "items = [] # 아이템 데이터를 넣을 리스트\n",
        "counter_max = 40\n",
        "\n",
        "for pg in page: # 원하는 페이지 수만큼 반복\n",
        "    resp = requests.get(f'https://browse.gmarket.co.kr/list?category=200000313&s=13&k=0&p={pg}') # s = 13 -> 리뷰 많은 순으로 정렬\n",
        "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
        "\n",
        "    item_name = soup.select('div#content div.box__item-title span.text__item')\n",
        "    item_url = soup.select(\"div#content span.text__item-title a.link__item\")\n",
        "    item_price = soup.select(\"div#content div.box__price-seller strong.text\")\n",
        "    counter = 0\n",
        "    # 아이템 테이블\n",
        "    for name,urls,prc in zip(item_name,item_url,item_price):\n",
        "        # 각 아이템별 내용을 저장할 딕셔너리 data\n",
        "        if counter < counter_max:\n",
        "            p_id += 1\n",
        "            data = {\n",
        "                    \"product_id\" : p_id, # 아이템 id를 지정\n",
        "                    \"product_name\": name['title'], # 아이템 이름\n",
        "                    \"price\": prc.text, # 가격\n",
        "                    \"url_link\" : urls['href'], # 아이템 별 세부페이지 url 소스\n",
        "            }\n",
        "            items.append(data)\n",
        "        counter += 1\n",
        "\n",
        "# 데이터 프레임 만들기\n",
        "item_frame = pd.DataFrame(items)\n",
        "\n",
        "###############################################\n",
        "\n",
        "headers = {\n",
        "        'Accept': '*/*',\n",
        "    'Accept-Encoding': 'gzip, deflate',\n",
        "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Content-Length': '42',\n",
        "    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "    }\n",
        "\n",
        "review_dic_list = []\n",
        "img_list = []\n",
        "detail_img_list = []\n",
        "\n",
        "\n",
        "for url,name, p_id in tqdm(zip(item_frame['url_link'], item_frame['product_name'], item_frame['product_id'])):\n",
        "    pg_num = 0\n",
        "\n",
        "    #프리미엄 상품평 최대 페이지 수 구하기\n",
        "    resp1 = requests.post('http://item.gmarket.co.kr/Review',\n",
        "                        data='goodsCode={}&pageNo=1&totalPage=1000'.format(url[41:]),\n",
        "                        headers=headers)\n",
        "    soup_page = BeautifulSoup(resp1.text)\n",
        "    page_number = soup_page.select('div#premium-wrapper div.board_pagenationwrap div.board_paging span.pagetotal em')\n",
        "    for pg in page_number:\n",
        "        pg_num = int(pg.text) # 최대 페이지 수 pg_num\n",
        "\n",
        "    ####################################\n",
        "\n",
        "    # 상품 썸네일 이미지 크롤링\n",
        "    resp_img = requests.get(f'http://item.gmarket.co.kr/Item?goodscode={url[41:51]}')\n",
        "    soup_img = BeautifulSoup(resp_img.text)\n",
        "    thumb_img = soup_img.select('div.thumb-gallery img')\n",
        "    for img in thumb_img:\n",
        "        img_list.append(img['src']) # 이미지 url 한개만 크롤링\n",
        "        break\n",
        "    ####################################\n",
        "\n",
        "    # 상세설명 이미지 크롤링\n",
        "    resp2 = requests.post('http://item.gmarket.co.kr/Item/ItemDetail',\n",
        "                            data='goodsCode={}'.format(url[41:]),\n",
        "                        headers=headers)\n",
        "    soup_detail_img = BeautifulSoup(resp2.content, 'html.parser')\n",
        "    i_img = soup_detail_img.select('div#basic_detail_html img')\n",
        "    detail_item_count = 0\n",
        "    for i in i_img:\n",
        "        detail_item_count += 1\n",
        "        # 몇번째 사진을 가져올 것 인가?\n",
        "        if detail_item_count == 1: # 1번째 사진\n",
        "            detail_img = i['src']\n",
        "            break\n",
        "    detail_img_dic = {\"detail_img\" : detail_img}\n",
        "    detail_img_list.append(detail_img_dic)\n",
        "    ####################################\n",
        "\n",
        "    # 프리미엄 리뷰 크롤링\n",
        "    for pgs in range(1, pg_num): # 1~20 페이지까지 크롤링(페이지 당 5개의 리뷰)\n",
        "        payload = {\n",
        "            'goodsCode': int(url[41:]), # 아이템 넘버\n",
        "            'pageNo': pgs, # 리뷰 페이지\n",
        "            'sort': 0, # 정렬방식\n",
        "            'totalPage': pg_num # 리뷰 페이지의 끝\n",
        "        } #payload를 통해 pgs와 url링크를 변환해서 접근\n",
        "\n",
        "        # Review/Premium에서 크롤링\n",
        "        resp3 = requests.post('http://item.gmarket.co.kr/Review/Premium',\n",
        "                            payload,\n",
        "                            headers=headers)\n",
        "        soup_review = BeautifulSoup(resp3.text)\n",
        "\n",
        "        reviews = soup_review.select('p.con') # 리뷰 텍스트\n",
        "        reviewer_ids = soup_review.select('dl.writer-info dd') #리뷰어 id\n",
        "        reviewer_id = reviewer_ids[::3] # reviewer_ids 를 크롤링 시에 날짜와 조회수가 같이 크롤링되어서 [::3]으로 id값만 추출\n",
        "\n",
        "        for review_id, review_text in zip(reviewer_id, reviews): # 페이지 당 5개의 리뷰\n",
        "            review_data = {\n",
        "                    \"PRODUCT_ID\" : p_id, #index\n",
        "                    \"PRODUCT_NAME\": name, # 아이템 명\n",
        "                    \"ID\" : review_id.text, #리뷰 유저 id\n",
        "                    \"REVIEW\": review_text.text #리뷰 텍스트\n",
        "            }\n",
        "            review_dic_list.append(review_data)\n",
        "\n",
        "#csv변환을 위한 데이터프레임화\n",
        "detail_img_df = pd.DataFrame(detail_img_list)\n",
        "thumb_img_df = pd.DataFrame(img_list)\n",
        "\n",
        "# 아이템 테이블에 이미지 데이터프레임 합치기\n",
        "item_frame_with_img = pd.concat([item_frame, thumb_img_df], axis = 1)\n",
        "item_frame_result = pd.concat([item_frame_with_img, detail_img_df], axis = 1)\n",
        "\n",
        "#이름 맞춰주기\n",
        "item_frame_result.rename(columns = {'product_id' : 'PRODUCT_ID', 'product_name': 'PRODUCT_NAME','price':'PRICE',\n",
        "                                    'url_link':'URL_LINK',0:'ITEM_IMG', 'detail_img': 'DETAIL_IMG'}, inplace = True)\n",
        "#csv로 파일만들어주기\n",
        "item_frame_result.to_csv(\"item_gmarket.csv\")\n",
        "\n",
        "review_dic_list = pd.DataFrame(review_dic_list)\n",
        "review_dic_list.to_csv(\"review_gmarket.csv\")"
      ],
      "metadata": {
        "id": "XT1xjruFt9Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#두 가지 카테고리 크롤링"
      ],
      "metadata": {
        "id": "TBmXhv21tQuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#크롤링 모델 > 2가지 카테고리 크롤링을 한 테이블에\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 데려올 카테고리 200001481 / 헤어스타일링 카테고리\n",
        "category = [200000313,200001481]\n",
        "page = [1] # page를 더 추가가 가능함.\n",
        "p_id = 0 # product_id를 넣을 변수\n",
        "\n",
        "counter_max = 20\n",
        "\n",
        "review_dic_list = []\n",
        "img_list = []\n",
        "detail_img_list = []\n",
        "items = [] # 아이템 데이터를 넣을 리스트\n",
        "\n",
        "for cat in category:\n",
        "    for pg in page: # 원하는 페이지 수만큼 반복\n",
        "        resp = requests.get(f'https://browse.gmarket.co.kr/list?category={cat}&s=13&k=0&p={pg}') # s = 13 -> 리뷰 많은 순으로 정렬\n",
        "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
        "\n",
        "        item_name = soup.select('div#content div.box__item-title span.text__item')\n",
        "        item_url = soup.select(\"div#content span.text__item-title a.link__item\")\n",
        "        item_price = soup.select(\"div#content div.box__price-seller strong.text\")\n",
        "        counter = 0\n",
        "        # 아이템 테이블\n",
        "        for name,urls,prc in zip(item_name,item_url,item_price):\n",
        "            # 각 아이템별 내용을 저장할 딕셔너리 data\n",
        "            if counter < counter_max:\n",
        "                p_id += 1\n",
        "                data = {\n",
        "                        \"CATEGORY_ID\" : cat,\n",
        "                        \"product_id\" : p_id, # 아이템 id를 지정\n",
        "                        \"product_name\": name['title'], # 아이템 이름\n",
        "                        \"price\": prc.text, # 가격\n",
        "                        \"url_link\" : urls['href'], # 아이템 별 세부페이지 url 소스\n",
        "                }\n",
        "                items.append(data)\n",
        "            counter += 1\n",
        "\n",
        "# 데이터 프레임 만들기\n",
        "item_frame = pd.DataFrame(items)\n",
        "\n",
        "###############################################\n",
        "\n",
        "headers = {\n",
        "        'Accept': '*/*',\n",
        "    'Accept-Encoding': 'gzip, deflate',\n",
        "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Content-Length': '42',\n",
        "    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "    }\n",
        "review_dic_list = []\n",
        "img_list = []\n",
        "detail_img_list = []\n",
        "\n",
        "for cat,url,name, p_id in tqdm(zip(item_frame['CATEGORY_ID'],item_frame['url_link'], item_frame['product_name'], item_frame['product_id'])):\n",
        "    pg_num = 0\n",
        "\n",
        "    #프리미엄 상품평 최대 페이지 수 구하기\n",
        "    resp1 = requests.post('http://item.gmarket.co.kr/Review',\n",
        "                        data='goodsCode={}&pageNo=1&totalPage=1000'.format(url[41:]),\n",
        "                        headers=headers)\n",
        "    soup_page = BeautifulSoup(resp1.text)\n",
        "    page_number = soup_page.select('div#premium-wrapper div.board_pagenationwrap div.board_paging span.pagetotal em')\n",
        "    for pg in page_number:\n",
        "        pg_num = int(pg.text) # 최대 페이지 수 pg_num\n",
        "\n",
        "    ####################################\n",
        "\n",
        "    # 상품 썸네일 이미지 크롤링\n",
        "    resp_img = requests.get(f'http://item.gmarket.co.kr/Item?goodscode={url[41:51]}')\n",
        "    soup_img = BeautifulSoup(resp_img.text)\n",
        "    thumb_img = soup_img.select('div.thumb-gallery img')\n",
        "    for img in thumb_img:\n",
        "        img_list.append(img['src']) # 이미지 url 한개만 크롤링\n",
        "        break\n",
        "    ####################################\n",
        "\n",
        "    # 상세설명 이미지 크롤링\n",
        "    resp2 = requests.post('http://item.gmarket.co.kr/Item/ItemDetail',\n",
        "                            data='goodsCode={}'.format(url[41:]),\n",
        "                        headers=headers)\n",
        "    soup_detail_img = BeautifulSoup(resp2.content, 'html.parser')\n",
        "    i_img = soup_detail_img.select('div#basic_detail_html img')\n",
        "    detail_item_count = 0\n",
        "    for i in i_img:\n",
        "        detail_item_count += 1\n",
        "        # 몇번째 사진을 가져올 것 인가?\n",
        "        if detail_item_count == 1: # 1번째 사진\n",
        "            detail_img = i['src']\n",
        "            break\n",
        "    detail_img_dic = {\"detail_img\" : detail_img}\n",
        "    detail_img_list.append(detail_img_dic)\n",
        "    ####################################\n",
        "\n",
        "    # 프리미엄 리뷰 크롤링\n",
        "    for pgs in range(1, pg_num): # 1~20 페이지까지 크롤링(페이지 당 5개의 리뷰)\n",
        "        payload = {\n",
        "            'goodsCode': int(url[41:]), # 아이템 넘버\n",
        "            'pageNo': pgs, # 리뷰 페이지\n",
        "            'sort': 0, # 정렬방식\n",
        "            'totalPage': pg_num # 리뷰 페이지의 끝\n",
        "        } #payload를 통해 pgs와 url링크를 변환해서 접근\n",
        "\n",
        "        # Review/Premium에서 크롤링\n",
        "        resp3 = requests.post('http://item.gmarket.co.kr/Review/Premium',\n",
        "                            payload,\n",
        "                            headers=headers)\n",
        "        soup_review = BeautifulSoup(resp3.text)\n",
        "\n",
        "        reviews = soup_review.select('p.con') # 리뷰 텍스트\n",
        "        reviewer_ids = soup_review.select('dl.writer-info dd') #리뷰어 id\n",
        "        reviewer_id = reviewer_ids[::3] # reviewer_ids 를 크롤링 시에 날짜와 조회수가 같이 크롤링되어서 [::3]으로 id값만 추출\n",
        "\n",
        "        for review_id, review_text in zip(reviewer_id, reviews): # 페이지 당 5개의 리뷰\n",
        "            review_data = {\n",
        "                    \"CATEGORY_ID\" : cat,\n",
        "                    \"PRODUCT_ID\" : p_id, #index\n",
        "                    \"PRODUCT_NAME\": name, # 아이템 명\n",
        "                    \"ID\" : review_id.text, #리뷰 유저 id\n",
        "                    \"REVIEW\": review_text.text #리뷰 텍스트\n",
        "            }\n",
        "            review_dic_list.append(review_data)\n",
        "\n",
        "#csv변환을 위한 데이터프레임화\n",
        "detail_img_df = pd.DataFrame(detail_img_list)\n",
        "thumb_img_df = pd.DataFrame(img_list)\n",
        "\n",
        "# 아이템 테이블에 이미지 데이터프레임 합치기\n",
        "item_frame_with_img = pd.concat([item_frame, thumb_img_df], axis = 1)\n",
        "item_frame_result = pd.concat([item_frame_with_img, detail_img_df], axis = 1)\n",
        "\n",
        "#이름 맞춰주기\n",
        "item_frame_result.rename(columns = {'product_id' : 'PRODUCT_ID', 'product_name': 'PRODUCT_NAME','price':'PRICE',\n",
        "                                    'url_link':'URL_LINK',0:'ITEM_IMG', 'detail_img': 'DETAIL_IMG'}, inplace = True)\n",
        "#csv로 파일만들어주기\n",
        "item_frame_result.to_csv(\"item_gmarket_hairstyle.csv\")\n",
        "\n",
        "review_dic_list = pd.DataFrame(review_dic_list)\n",
        "review_dic_list.to_csv(\"review_gmarket_hairstyle.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXZu68lRZaXD",
        "outputId": "3e5314ff-4e68-46fc-9218-0f618dec8941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "40it [1:21:23, 122.08s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#크롤링 모델(아이템 테이블만)\n"
      ],
      "metadata": {
        "id": "cIWGXojFl3dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#크롤링 모델 > 아이템 테이블\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 데려올 카테고리 200000313 => 헤어케어\n",
        "page = [1]\n",
        "p_id = 0\n",
        "items = []\n",
        "\n",
        "for pg in page:\n",
        "    resp = requests.get(f'https://browse.gmarket.co.kr/list?category=200000313&s=13&k=0&p={pg}') #s = 13 --> 리뷰 많은 순.\n",
        "    # 크롤링\n",
        "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
        "\n",
        "    item_name = soup.select('div#content div.box__item-title span.text__item')\n",
        "    item_url = soup.select(\"div#content span.text__item-title a.link__item\") # url\n",
        "    item_price = soup.select(\"div#content div.box__price-seller strong.text\")\n",
        "\n",
        "    # 아이템 설명 테이블\n",
        "    for name,urls,prc in zip(item_name,item_url,item_price):\n",
        "        # 각 아이템별 내용을 저장할 딕셔너리 data\n",
        "        p_id += 1\n",
        "        data = {\n",
        "                \"product_id\" : p_id, #index\n",
        "                \"product_name\": name['title'], # 이름\n",
        "                \"price\": prc.text, # 가격\n",
        "                \"url_link\" : urls['href'], # 아이템 별 세부페이지 url 소스\n",
        "        }\n",
        "        items.append(data)\n",
        "\n",
        "# 데이터 프레임 만들기\n",
        "item_frame = pd.DataFrame(items)\n",
        "\n",
        "headers = {\n",
        "        'Accept': '*/*',\n",
        "    'Accept-Encoding': 'gzip, deflate',\n",
        "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Content-Length': '42',\n",
        "    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "    }\n",
        "\n",
        "img_list = []\n",
        "detail_img_list = []\n",
        "\n",
        "for url,name, p_id in tqdm(zip(item_frame['url_link'], item_frame['product_name'], item_frame['product_id'])): #리뷰 데이터 모으려고 만든 것이라서 불필요한 매개변수 존재\n",
        "    pg_num = 0\n",
        "    review_list = []\n",
        "    review_id_list = []\n",
        "\n",
        "    resp_img = requests.get(f'http://item.gmarket.co.kr/Item?goodscode={url[41:51]}')\n",
        "    soup_img = BeautifulSoup(resp_img.text)\n",
        "    thumb_img = soup_img.select('div.thumb-gallery img')\n",
        "\n",
        "    for img in thumb_img:\n",
        "        img_list.append(img['src'])\n",
        "        break\n",
        "\n",
        "    # 상세설명 이미지 크롤링\n",
        "    resp2 = requests.post('http://item.gmarket.co.kr/Item/ItemDetail',\n",
        "                            data='goodsCode={}'.format(url[41:51]),\n",
        "                        headers=headers)\n",
        "    soup_detail_img = BeautifulSoup(resp2.content, 'html.parser')\n",
        "    i_img = soup_detail_img.select('div#basic_detail_html img')\n",
        "    detail_item_count = 0\n",
        "    for i in i_img:\n",
        "        detail_item_count += 1\n",
        "        # 몇번째 사진을 가져올 것 인가?\n",
        "        if detail_item_count == 1: # 1번째 사진\n",
        "            detail_img = i['src']\n",
        "            break\n",
        "    detail_img_dic = {\"detail_img\" : detail_img}\n",
        "    detail_img_list.append(detail_img_dic)\n",
        "\n",
        "detail_img_df = pd.DataFrame(detail_img_list)\n",
        "thumb_img_df = pd.DataFrame(img_list)\n",
        "\n",
        "item_frame_with_img = pd.concat([item_frame, thumb_img_df], axis = 1)\n",
        "item_frame_result = pd.concat([item_frame_with_img, detail_img_df], axis = 1)\n",
        "\n",
        "item_frame_result.rename(columns = {'product_id' : 'PRODUCT_ID', 'product_name': 'PRODUCT_NAME','price':'PRICE',\n",
        "                                    'url_link':'URL_LINK',0:'ITEM_IMG', 'detail_img': 'DETAIL_IMG'}, inplace = True)\n",
        "item_frame_result.to_csv(\"item_gmarket.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkSKeXNGJD9p",
        "outputId": "63fa6b50-56d4-4be0-be35-0521a023f0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [05:34,  3.35s/it]\n"
          ]
        }
      ]
    }
  ]
}