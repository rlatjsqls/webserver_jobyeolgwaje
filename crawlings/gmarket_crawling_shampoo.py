# -*- coding: utf-8 -*-
"""Gmarket_crawling_shampoo

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xDiCUOGErUK3Ep1G_HGaWxfYHXaiOx4j
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from tqdm import tqdm

# 데려올 카테고리 200000313 = 헤어케어
page = [1]
p_id = 0
items = []

for pg in page:
    resp = requests.get(f'https://browse.gmarket.co.kr/list?category=200000313&s=13&k=0&p={pg}') #s = 13 --> 리뷰 많은 순.
    # 크롤링
    soup = BeautifulSoup(resp.content, 'html.parser')

    item_name = soup.select('div#content div.box__item-title span.text__item')
    item_url = soup.select("div#content a.link__item") # url
    item_price = soup.select("div#content div.box__price-seller strong.text")
    # 아이템 설명 테이블
    for name,url,prc in zip(item_name,item_url,item_price):
        # 각 아이템별 내용을 저장할 딕셔너리 data
        data = {
                "product_id" : p_id, #index
                "product_name": name['title'], # 이름
                "price": prc.text, # 가격
                "url_link" : url['href'], # 아이템 별 세부페이지 url 소스
            }
        p_id += 1
        items.append(data)

    # 데이터 프레임 만들기
item_frame = pd.DataFrame(items)

item_frame.to_csv("item_data_gmarket.csv")

headers = {
        'Accept': '*/*',
    'Accept-Encoding': 'gzip, deflate',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive',
    'Content-Length': '42',
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    }

review_dic_list = []

for url,name, p_id in tqdm(zip(item_frame['url_link'], item_frame['product_name'], item_frame['product_id'])):
    pg_num = 0
    review_list = []
    review_id_list = []

    #프리미엄 상품평 최대 페이지 수 구하기
    resp = requests.post('http://item.gmarket.co.kr/Review',
                        data='goodsCode={}&pageNo=1&totalPage=1000'.format(url[41:51]),
                        headers=headers)
    soup_page = BeautifulSoup(resp.text)
    page_number = soup_page.select('div#premium-wrapper div.board_pagenationwrap div.board_paging span.pagetotal em')

    for pg in page_number:
        pg_num = pg.text
    #####################################
    for pg in range(1,5):
        resp1 = requests.post('http://item.gmarket.co.kr/Review',
                            data='goodsCode={}&pageNo={}&totalPage=1000'.format(url[41:51],pg),
                            headers=headers)
        soup = BeautifulSoup(resp1.text)

        reviews = soup.select('div#premium-wrapper table.tb_comment td.comment-content a.uxelayer_ctrl p.con')
        reviewer_id = soup.select('div#premium-wrapper table.tb_comment dl.writer-info dd')

        for r_i in reviewer_id:
            review_id_list.append(r_i.text)
        review_id_lists = review_id_list[::3]

        for review_id, review_text in zip(review_id_lists, reviews): # 5번 돌아감.
            review_data = {
                    "product_id" : p_id, #index
                    "product_name": name, # 아이템 명
                    "reviewer_id" : review_id, #리뷰 유저 id
                    "review_text": review_text.text
            }
            review_dic_list.append(review_data)

review_dic_list = pd.DataFrame(review_dic_list)
review_dic_list.to_csv("review_gmarket.csv")

# import requests
# from bs4 import BeautifulSoup
# import pandas as pd
# from tqdm import tqdm

# check = 0
# check_idx = 0
# p_id = 0
# # 데려올 카테고리 G07(식품)
# cat_list = ['G01','G02','G03','G04','G05','G06','G07','G08','G09','G10','G11','G12']
# for cat in cat_list:
#     resp = requests.get(f'https://www.gmarket.co.kr/n/best?viewType=G&groupCode={cat}')

#     # 크롤링
#     soup = BeautifulSoup(resp.content)
#     print(soup)
#     break
#     cs = soup.select("div.best-list ul a.itemname") # url / 이름
#     imgs = soup.select("div.best-list ul div.thumb img.image__lazy") # 이미지
#     price = soup.select("div.best-list ul div.item_price div.s-price")

#     #item이라는 리스트 생성
#     items = []
#     p_id_list = []

#     # 아이템 설명 테이블
#     for info,img,prc in zip(cs,imgs,price):
#         # 각 아이템별 내용을 저장할 딕셔너리 data
#         data = {
#                 "product_id" : p_id,
#                 "category_id" : cat, # 카테고리
#                 "thumb_img" : img['src'], # 썸네일 이미지
#                 "product_name": info.text, # 이름
#                 "price": prc.select_one("strong > span").text, # 가격
#                 "url_link" : info['href'], # 아이템 별 세부페이지 url 소스
#             }
#         p_id_list.append(p_id)
#         p_id += 1
#         items.append(data)

#     # 데이터 프레임 만들기
#     item_frame = pd.DataFrame(items)

#     headers = {
#         'Accept': '*/*',
#     'Accept-Encoding': 'gzip, deflate',
#     'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
#     'Connection': 'keep-alive',
#     'Content-Length': '42',
#     'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
#     }

#     # 아이템 리뷰 테이블을 저장할 리스트
#     item_review_contents = []
#     #아이템 상세 사진을 저장할 리스트
#     detail_img_list = []

#     # 각 url 링크에 접근하여 리뷰 데이터 (카테고리 당 100개 아이템 x 10개 리뷰)
#     for url,name, p_id in tqdm(zip(item_frame['url_link'], item_frame['product_name'], item_frame['product_id'])):
#         # 리뷰 크롤링
#         resp1 = requests.post('http://item.gmarket.co.kr/Review/Text',
#                         data='goodsCode={}&pageNo=1&totalPage=100'.format(url[41:51]),
#                         headers=headers)
#         soup = BeautifulSoup(resp1.text)
#         reviews = soup.select('table p.con') # 리뷰(일반 상품평)
#         ids = soup.select('table dd')[::2] # 리뷰를 남긴 사용자 id
#         recommends = soup.select('table td.comment-grade span.rec') # 평점

#         # 상세설명 이미지 크롤링
#         resp2 = requests.post('http://item.gmarket.co.kr/Item/ItemDetail',
#                               data='goodsCode={}'.format(url[41:51]),
#                             headers=headers)
#         soup_img = BeautifulSoup(resp2.content, 'html.parser')
#         i_img = soup_img.select('div#basic_detail_html img')

#         detail_item_count = 0
#         for i in i_img:
#             detail_item_count += 1
#             # 몇번째 사진을 가져올 것 인가?
#             if detail_item_count == 1: # 1번째 사진
#                 detail_img = i['src']
#                 break

#         detail_img_dic = {
#             "detail_img" : detail_img,
#             }
#         detail_img_list.append(detail_img_dic)
#         detail_img_df = pd.DataFrame(detail_img_list)

#         item_frame_with_detail_img = pd.concat([item_frame, detail_img_df], axis = 1)

#         for review,id,recommend in zip(reviews, ids, recommends):
#             item_review_dic = {
#                 "product_idx" : p_id, # 아이템 id
#                 "product_name" : name, # 아이템 명
#                 "review_content" : review.text.strip(), # 리뷰 텍스트
#                 "reviewer_id" : id.text.strip(), # 리뷰작성자 id
#                 "recommend_grade" : recommend.text.strip(), # 평점
#             }
#             item_review_contents.append(item_review_dic)

#     item_review_frame = pd.DataFrame(item_review_contents)

#     check += 1 # 카테고리 별 내용을 합치기 위한 check / 1번째 df를 제외하곤 아래에다가 붙일 생각.

#     if check == 1:
#         result_review_frame = item_review_frame
#         result_item_frame = item_frame_with_detail_img
#     else:
#         result_review_frame = pd.concat([result_review_frame, item_review_frame], ignore_index=True)
#         result_item_frame = pd.concat([result_item_frame, item_frame_with_detail_img], ignore_index=True)
#     check_idx += 1

# #csv로 변환
# result_item_frame.to_csv("item_table_gmarket.csv")

# # 평점 데이터를 수치형으로 변경
# review_frame_chg = result_review_frame
# review_frame_chg.loc[review_frame_chg['recommend_grade'] == "적극추천", 'recommend_grade'] = 5
# review_frame_chg.loc[review_frame_chg['recommend_grade'] == "추천", 'recommend_grade'] = 3
# review_frame_chg.loc[review_frame_chg['recommend_grade'] == "보통", 'recommend_grade'] = 1
# review_frame_chg.loc[review_frame_chg['recommend_grade'] == "추천안함", 'recommend_grade'] = 0

# #최종 리뷰데이터
# review_frame_chg.to_csv("review_table_gmarket.csv")

